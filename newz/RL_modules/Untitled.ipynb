{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d74cb48-0037-458b-9f61-65e92515a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 12:47:40.427660: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-17 12:47:40.428643: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 12:47:40.431096: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 12:47:40.438591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752736660.449953  685381 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752736660.453140  685381 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752736660.462918  685381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752736660.462938  685381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752736660.462939  685381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752736660.462940  685381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-17 12:47:40.466781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "E0000 00:00:1752736691.296456  685381 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1752736691.296727  685381 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- Model creation ---\n",
    "def create_cnn_dqn(action_size=4):\n",
    "    grid_input = tf.keras.Input(shape=(20, 20, 1), name='grid_input')\n",
    "    x1 = tf.keras.layers.Conv2D(16, 3, activation='tanh', padding='same')(grid_input)\n",
    "    x1 = tf.keras.layers.MaxPooling2D(2)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "    pose_input = tf.keras.Input(shape=(4,), name='pose_input')\n",
    "    x2 = tf.keras.layers.Dense(32, activation='relu')(pose_input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(action_size, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[grid_input, pose_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
    "    return model\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "ACTION_SIZE = 4\n",
    "EPISODES = 1000\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.05\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "\n",
    "# --- Initialize networks and memory ---\n",
    "policy_net = create_cnn_dqn(ACTION_SIZE)\n",
    "target_net = create_cnn_dqn(ACTION_SIZE)\n",
    "target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "dummy_grid = np.zeros((1, 20, 20, 1), dtype=np.float32)\n",
    "dummy_pose = np.zeros((1, 4), dtype=np.float32)\n",
    "policy_net.predict([dummy_grid, dummy_pose])\n",
    "target_net.predict([dummy_grid, dummy_pose])\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6949f6-ce4a-4764-b68e-1b68c953e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_local_map(global_map, robot_pose, size=20):\n",
    "    half = size // 2\n",
    "    r, c = robot_pose\n",
    "    local_map = np.full((size, size), 1, dtype=np.int8)  # initialize with padding value 1\n",
    "\n",
    "    for i in range(-half, half):\n",
    "        for j in range(-half, half):\n",
    "            gi, gj = r + i, c + j\n",
    "            li, lj = i + half, j + half  # local map indices\n",
    "            if 0 <= gi < global_map.shape[0] and 0 <= gj < global_map.shape[1]:\n",
    "                local_map[li, lj] = global_map[gi, gj]\n",
    "            # else: keep the padding value = 1\n",
    "\n",
    "    return local_map\n",
    "\n",
    "\n",
    "def safe_move(robot_pose, action_map,action, rows, cols):\n",
    "    flag = True\n",
    "\n",
    "    while flag:\n",
    "      move = action_map[action]\n",
    "      new_x = robot_pose[0] + move[0]\n",
    "      new_y = robot_pose[1] + move[1]\n",
    "      if 0 <= new_x < rows and 0 <= new_y < cols:\n",
    "          flag = False\n",
    "          return [new_x, new_y]\n",
    "      else:\n",
    "        action = random.randint(0,3)  # or return unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5311438-eaf2-44c6-87dc-6d40e997b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(robot_pose, goal_pose, action,collided, reached_goal, previous_distance, current_distance, explored_cells,visited_set,occ_map,recent_pos):\n",
    "    reward = 0.0\n",
    "    #print(previous_distance,current_distance)\n",
    "    if recent_pos.count(tuple(robot_pose)) >=1:\n",
    "        reward = reward - 20* recent_pos.count(tuple(robot_pose))\n",
    "    # if tuple(robot_pose) in visited_set:\n",
    "    #     reward = reward -30;\n",
    "    # else:\n",
    "    #     visited_set.add(tuple(robot_pose))\n",
    "    #     reward = reward +10;\n",
    "    if robot_pose[0] == goal_pose[0]:\n",
    "        reward = reward +200;\n",
    "        if (goal_pose[1]-robot_pose[1] > 0):\n",
    "            if (action[1] == 1):\n",
    "                reward += 200;\n",
    "            else:\n",
    "                reward -= 100;\n",
    "        if (goal_pose[1]-robot_pose[1] < 0):\n",
    "            if (action[1] == -1):\n",
    "                reward += 200;\n",
    "            else:\n",
    "                reward -= 100;\n",
    "    if robot_pose[1] == goal_pose[1]:\n",
    "        reward = reward +200;\n",
    "        if (goal_pose[0]-robot_pose[0] > 0):\n",
    "            if (action[0] == 1):\n",
    "                reward += 200;\n",
    "            else:\n",
    "                reward -= 100;\n",
    "        if (goal_pose[0]-robot_pose[0] < 0):\n",
    "            if (action[0] == -1):\n",
    "                reward += 200;\n",
    "            else:\n",
    "                reward -= 100;\n",
    "    if collided:\n",
    "        return reward -1000.0;  # Large negative for hitting wall or obstacle\n",
    "\n",
    "    if reached_goal:\n",
    "        return reward +1000.0; # Goal reached\n",
    "\n",
    "    if (57< robot_pose[0] | robot_pose[0]<3 ):\n",
    "        reward = reward-20;\n",
    "    else:\n",
    "        reward= reward +10;\n",
    "    if (57< robot_pose[1] | robot_pose[1]< 3):\n",
    "        reward = reward -20;\n",
    "    else:\n",
    "        reward = reward +10;\n",
    "    # Reward getting closer to goal\n",
    "    if current_distance < previous_distance:\n",
    "        reward += 20.0\n",
    "    else:\n",
    "        reward -= 20.0  # Penalize going away or staying same\n",
    "\n",
    "    # Small penalty to discourage taking too long\n",
    "    reward -= 5\n",
    "    reward = reward + 60 - np.abs(goal_pose[0]-robot_pose[0])\n",
    "    reward = reward + 60 - np.abs(goal_pose[1]-robot_pose[1])\n",
    "\n",
    "    # Bonus for exploring new cells in occupancy grid\n",
    "    reward += 0.1 * explored_cells  # Optional if tracking exploration\n",
    "\n",
    "    dx, dy = action\n",
    "    isclear = True\n",
    "    for step in range(1, 11):  # Look 10 cells ahead\n",
    "        nx, ny = robot_pose[0] + dx * step, robot_pose[1] + dy * step\n",
    "        if 0 <= nx < occ_map.shape[0] and 0 <= ny < occ_map.shape[1]:\n",
    "            if occ_map[nx, ny] >= 1:  # Obstacle detected\n",
    "                reward -= 10*(10- step)  # Penalty for heading toward obstacle\n",
    "                isclear= False\n",
    "                break  # Stop after first obstacle detected\n",
    "        else:\n",
    "            reward -= 5  # Penalty for moving out of bounds\n",
    "            break\n",
    "    if(isclear):\n",
    "        reward = reward+5\n",
    "    \n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488828f1-37be-4bae-b071-7f024f2b9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(state, action, goal, direction, huristic, explored, recent_pos):\n",
    "    \"\"\"\n",
    "    Simulates one environment step.\n",
    "    \n",
    "    Parameters:\n",
    "        state          : global map (2D numpy array)\n",
    "        action         : new robot position [x, y]\n",
    "        goal           : goal position [x, y]\n",
    "        direction      : movement direction (dx, dy)\n",
    "        huristic       : previous distance to goal\n",
    "        explored       : set of visited positions\n",
    "        recent_pos     : deque of recent positions\n",
    "    \n",
    "    Returns:\n",
    "        next_state     : new 20x20 local map (1, 20, 20, 1)\n",
    "        reward         : reward for the step\n",
    "        done           : whether the episode should terminate\n",
    "    \"\"\"\n",
    "    # Generate new local map from global state\n",
    "    next_map = get_local_map(state, action, size=20)\n",
    "    next_state = next_map.astype(np.float32).reshape(1, 20, 20, 1)\n",
    "\n",
    "    # Check for collision\n",
    "    collided = state[action[0], action[1]] >= 1\n",
    "\n",
    "    # Check if goal is reached\n",
    "    reached_goal = (action[0] == goal[0]) and (action[1] == goal[1])\n",
    "\n",
    "    # Compute distance to goal\n",
    "    previous_distance = huristic\n",
    "    current_distance = np.sqrt(\n",
    "        np.square(goal[0] - action[0]) + np.square(goal[1] - action[1])\n",
    "    )\n",
    "\n",
    "    # Compute reward (you must define this function separately)\n",
    "    # reward = compute_reward(\n",
    "    #     action, goal, direction, collided, reached_goal,\n",
    "    #     previous_distance, current_distance,\n",
    "    #     10, explored=explored,\n",
    "    #     state=state, recent_pos=recent_pos\n",
    "    # )\n",
    "    reward = compute_reward(action, goal,direction, collided, reached_goal, previous_distance, current_distance, 10,explored,state,recent_pos)\n",
    "\n",
    "    # Done if collided or reached goal\n",
    "    done = reached_goal or collided\n",
    "\n",
    "    return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2c9f-d044-4c17-861e-04dad5b908d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved checkpoint at episode 0\n",
      "Episode 1/1000 - Total reward: -310.00 - Epsilon: 0.99\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 2/1000 - Total reward: -310.00 - Epsilon: 0.99\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 3/1000 - Total reward: 13411.00 - Epsilon: 0.99\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 4/1000 - Total reward: 37368.00 - Epsilon: 0.98\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 5/1000 - Total reward: 224551.00 - Epsilon: 0.98\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 6/1000 - Total reward: 225489.00 - Epsilon: 0.97\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 7/1000 - Total reward: 281601.00 - Epsilon: 0.97\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 8/1000 - Total reward: 26609.00 - Epsilon: 0.96\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 9/1000 - Total reward: 26609.00 - Epsilon: 0.96\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 10/1000 - Total reward: 26609.00 - Epsilon: 0.95\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 11/1000 - Total reward: 26609.00 - Epsilon: 0.95\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 12/1000 - Total reward: 26609.00 - Epsilon: 0.94\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 13/1000 - Total reward: 319196.00 - Epsilon: 0.94\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 14/1000 - Total reward: 27009.00 - Epsilon: 0.93\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 15/1000 - Total reward: 27009.00 - Epsilon: 0.93\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 16/1000 - Total reward: 27009.00 - Epsilon: 0.92\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 17/1000 - Total reward: 27009.00 - Epsilon: 0.92\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 18/1000 - Total reward: 27009.00 - Epsilon: 0.91\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 19/1000 - Total reward: 27009.00 - Epsilon: 0.91\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 20/1000 - Total reward: 27009.00 - Epsilon: 0.90\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 21/1000 - Total reward: 27009.00 - Epsilon: 0.90\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 22/1000 - Total reward: 27009.00 - Epsilon: 0.90\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 23/1000 - Total reward: 27009.00 - Epsilon: 0.89\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 24/1000 - Total reward: 27009.00 - Epsilon: 0.89\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 25/1000 - Total reward: 27009.00 - Epsilon: 0.88\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 26/1000 - Total reward: 27009.00 - Epsilon: 0.88\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 27/1000 - Total reward: 27009.00 - Epsilon: 0.87\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 28/1000 - Total reward: 27009.00 - Epsilon: 0.87\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 29/1000 - Total reward: 27009.00 - Epsilon: 0.86\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 30/1000 - Total reward: 27009.00 - Epsilon: 0.86\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 31/1000 - Total reward: 27009.00 - Epsilon: 0.86\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 32/1000 - Total reward: 27009.00 - Epsilon: 0.85\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 33/1000 - Total reward: 27009.00 - Epsilon: 0.85\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 34/1000 - Total reward: 27009.00 - Epsilon: 0.84\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 35/1000 - Total reward: 27009.00 - Epsilon: 0.84\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 36/1000 - Total reward: 27009.00 - Epsilon: 0.83\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 37/1000 - Total reward: 27009.00 - Epsilon: 0.83\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 38/1000 - Total reward: 27009.00 - Epsilon: 0.83\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 39/1000 - Total reward: 27009.00 - Epsilon: 0.82\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 40/1000 - Total reward: 27009.00 - Epsilon: 0.82\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 41/1000 - Total reward: 27009.00 - Epsilon: 0.81\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 42/1000 - Total reward: 27009.00 - Epsilon: 0.81\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 43/1000 - Total reward: 27009.00 - Epsilon: 0.81\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 44/1000 - Total reward: 27009.00 - Epsilon: 0.80\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 45/1000 - Total reward: 27009.00 - Epsilon: 0.80\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 46/1000 - Total reward: 27009.00 - Epsilon: 0.79\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 47/1000 - Total reward: 27009.00 - Epsilon: 0.79\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 48/1000 - Total reward: 27009.00 - Epsilon: 0.79\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 49/1000 - Total reward: 27009.00 - Epsilon: 0.78\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 50/1000 - Total reward: 27009.00 - Epsilon: 0.78\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved checkpoint at episode 50\n",
      "Episode 51/1000 - Total reward: 27009.00 - Epsilon: 0.77\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 52/1000 - Total reward: 27009.00 - Epsilon: 0.77\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 53/1000 - Total reward: 27009.00 - Epsilon: 0.77\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 54/1000 - Total reward: 27009.00 - Epsilon: 0.76\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 55/1000 - Total reward: 27009.00 - Epsilon: 0.76\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 56/1000 - Total reward: 27009.00 - Epsilon: 0.76\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 57/1000 - Total reward: 27009.00 - Epsilon: 0.75\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 58/1000 - Total reward: 27009.00 - Epsilon: 0.75\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 59/1000 - Total reward: 27009.00 - Epsilon: 0.74\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 60/1000 - Total reward: 27009.00 - Epsilon: 0.74\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 61/1000 - Total reward: 27009.00 - Epsilon: 0.74\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 62/1000 - Total reward: 30861.00 - Epsilon: 0.73\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n",
      "Episode 63/1000 - Total reward: 27009.00 - Epsilon: 0.73\n",
      "pair id: 0 robo: (5, 5) goal: (8, 55)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "MAX_STEPS_EP = 3600\n",
    "\n",
    "global_map = np.zeros((60, 60), dtype=np.int8)\n",
    "global_map[10:15, 10:50] = 1\n",
    "global_map[30:35, 5:55] = 1\n",
    "global_map[41:42, 41:42 ] = 1\n",
    "\n",
    "explored = set()\n",
    "\n",
    "robot_pose = [5, 5]\n",
    "goal_pose = [40, 40]\n",
    "\n",
    "# Define exactly 10 robot–goal pairs (manually indexed)\n",
    "robot_goal_map = {\n",
    "    0: ((5, 5), (8, 55)),\n",
    "    1: ((5, 5), (50,3)),\n",
    "    2: ((5, 5), (25, 25)),\n",
    "    3: ((8, 8), (38, 42)),\n",
    "    4: ((5, 55), (50, 20)),\n",
    "    5: ((25, 25), (5, 5)),\n",
    "    6: ((8, 30), (45, 10)),\n",
    "    7: ((55, 55), (20, 10)),\n",
    "    8: ((50, 30), (5, 30)),\n",
    "    9: ((3, 40), (40, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1) # right\n",
    "}\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # --- Reset environment ---\n",
    "    #print(robot_pose)\n",
    "    pair_id = episode // 100\n",
    "    robot_pose, goal_pose = robot_goal_map[pair_id]\n",
    "    print(\"pair id: \" + str(pair_id) + \" robo: \" + str(robot_pose) + \" goal: \" + str(goal_pose))\n",
    "    robot_pose = list(robot_pose)\n",
    "    goal_pose = list(goal_pose)\n",
    "    recent_pos = deque(maxlen=20)\n",
    "    local_map = get_local_map(global_map, robot_pose, size=20) # np.random.randint(0, 2, (20, 20, 1)).astype(np.float32)  # Example\n",
    "    #print(local_map.shape)\n",
    "    state_grid = local_map.astype(np.float32).reshape(1, 20, 20, 1)\n",
    "   # state_grid = local_map.astype(np.float32).transpose(0, 2, 3, 1)\n",
    "    state_pose = np.array([[robot_pose[0], robot_pose[1], goal_pose[0], goal_pose[1]]], dtype=np.float32)             # Example\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    #print(\"rbpose\",robot_pose)\n",
    "    step_count = 0\n",
    "    while not done and step_count<=MAX_STEPS_EP:\n",
    "        step_count += 1\n",
    "        # --- Epsilon-greedy action selection ---\n",
    "        # if np.random.rand() <= EPSILON:\n",
    "        #     action = random.randint(0, ACTION_SIZE - 1)\n",
    "        # else:\n",
    "        #     q_vals = policy_net.predict([state_grid[np.newaxis, ...], state_pose[np.newaxis, ...]], verbose=0)\n",
    "        #     action = np.argmax(q_vals[0])\n",
    "\n",
    "        # --- Take action in environment ---\n",
    "        #print(\"current\", robot_pose)\n",
    "        huristic = np.sqrt(np.square(goal_pose[0] - robot_pose[0]) + np.square(goal_pose[1] - robot_pose[1]))\n",
    "        q_values = policy_net.predict([state_grid, state_pose], verbose=0)\n",
    "        action = np.argmax(q_values[0])\n",
    "        move = action_map[action]\n",
    "       # print(move)\n",
    "       # robot_pose[0] = robot_pose[0] + move[0]\n",
    "       # robot_pose[1] = robot_pose[1] + move[1]\n",
    "        robot_pose = safe_move(robot_pose,action_map,action,60,60)\n",
    "        recent_pos.append(tuple(robot_pose))\n",
    "        next_grid, reward, done = env_step(global_map, robot_pose,goal_pose,move,huristic,explored,recent_pos)  # Placeholder\n",
    "        next_pose = np.array([[robot_pose[0], robot_pose[1], goal_pose[0], goal_pose[1]]], dtype=np.float32)\n",
    "        #print(robot_pose,reward)\n",
    "        memory.append((state_grid, state_pose, action, reward, next_grid, next_pose, done))\n",
    "\n",
    "        state_grid, state_pose = next_grid, next_pose\n",
    "        total_reward += reward\n",
    "\n",
    "        # --- Train if enough samples ---\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            minibatch = random.sample(memory, BATCH_SIZE)\n",
    "\n",
    "            grids = np.array([x[0] for x in minibatch])\n",
    "            poses = np.array([x[1] for x in minibatch])\n",
    "            actions = np.array([x[2] for x in minibatch])\n",
    "            rewards = np.array([x[3] for x in minibatch])\n",
    "            next_grids = np.array([x[4] for x in minibatch])\n",
    "            next_poses = np.array([x[5] for x in minibatch])\n",
    "            dones = np.array([x[6] for x in minibatch])\n",
    "\n",
    "          #  print(\"grids shape:\", grids.shape)   # Should be (batch_size, 20, 20, 1)\n",
    "           # print(\"poses shape:\", poses.shape)   # Should be (batch_size, 4)\n",
    "\n",
    "            q_targets = policy_net.predict([grids.reshape(-1,20,20,1), poses.reshape(-1,4)], verbose=0)\n",
    "            q_next = target_net.predict([next_grids.reshape(-1,20,20,1), next_poses.reshape(-1,4)], verbose=0)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                target = rewards[i]\n",
    "                if not dones[i]:\n",
    "                    target += GAMMA * np.amax(q_next[i])\n",
    "                q_targets[i][actions[i]] = target\n",
    "\n",
    "            # --- Train policy network ---\n",
    "            policy_net.fit([grids.reshape(-1,20,20,1), poses.reshape(-1,4)], q_targets, verbose=0)\n",
    "\n",
    "    # --- Update target network ---\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "    # --- Epsilon decay ---\n",
    "    if EPSILON > EPSILON_MIN:\n",
    "        EPSILON *= EPSILON_DECAY\n",
    "    if episode % 50 == 0:\n",
    "        policy_net.save(f\"/storage/projects2/e19-4yp-g28-peraswarm-local-nav/checkpoint_ep{episode}.h5\")\n",
    "        print(f\" Saved checkpoint at episode {episode}\")\n",
    "    print(f\"Episode {episode+1}/{EPISODES} - Total reward: {total_reward:.2f} - Epsilon: {EPSILON:.2f}\")\n",
    "    \n",
    "policy_net.save(\"path_planing_model_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fb8c4-b434-499d-b2a8-23c144a3a840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
