{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e8842-19e8-4f00-889a-752e1fb1c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def create_drone_dqn(action_size=6):\n",
    "    # 3D grid input\n",
    "    grid_input = tf.keras.Input(shape=(20, 20, 20, 1), name='grid_input')\n",
    "    x1 = tf.keras.layers.Conv3D(16, 3, activation='relu', padding='same')(grid_input)\n",
    "    x1 = tf.keras.layers.MaxPooling3D(2)(x1)\n",
    "    x1 = tf.keras.layers.Conv3D(32, 3, activation='relu', padding='same')(x1)\n",
    "    x1 = tf.keras.layers.MaxPooling3D(2)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "    # Pose + goal input\n",
    "    pose_input = tf.keras.Input(shape=(6,), name='pose_input')\n",
    "    x2 = tf.keras.layers.Dense(64, activation='relu')(pose_input)\n",
    "    x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
    "\n",
    "    # Combine\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(action_size, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[grid_input, pose_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37750f02-e56b-40b2-ba04-3f565c3704a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SIZE = 6  # Updated\n",
    "EPISODES = 1000\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.05\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "\n",
    "policy_net = create_drone_dqn(ACTION_SIZE)\n",
    "target_net = create_drone_dqn(ACTION_SIZE)\n",
    "target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "dummy_grid = np.zeros((1, 20, 20, 20, 1), dtype=np.float32)\n",
    "dummy_pose = np.zeros((1, 6), dtype=np.float32)\n",
    "\n",
    "policy_net.predict([dummy_grid, dummy_pose])\n",
    "target_net.predict([dummy_grid, dummy_pose])\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7977c-0cbc-425c-98f8-075425468b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_map_3d(global_map, robot_pose, size=20):\n",
    "    half = size // 2\n",
    "    x, y, z = robot_pose\n",
    "    local_map = np.full((size, size, size), 1, dtype=np.int8)  # pad with 1s (obstacles)\n",
    "\n",
    "    for i in range(-half, half):\n",
    "        for j in range(-half, half):\n",
    "            for k in range(-half, half):\n",
    "                gi, gj, gk = x + i, y + j, z + k\n",
    "                li, lj, lk = i + half, j + half, k + half  # local map indices\n",
    "\n",
    "                if (0 <= gi < global_map.shape[0] and\n",
    "                    0 <= gj < global_map.shape[1] and\n",
    "                    0 <= gk < global_map.shape[2]):\n",
    "                    local_map[li, lj, lk] = global_map[gi, gj, gk]\n",
    "                # else: keep as 1 (padding/obstacle)\n",
    "    \n",
    "    return local_map\n",
    "import random\n",
    "\n",
    "def safe_move_3d(robot_pose, action_map, action, shape):\n",
    "    \"\"\"\n",
    "    shape: (rows, cols, depth) of the 3D map\n",
    "    \"\"\"\n",
    "    max_x = shape\n",
    "    max_y = shape\n",
    "    max_z = shape\n",
    "    flag = True\n",
    "\n",
    "    while flag:\n",
    "        dx, dy, dz = action_map[action]\n",
    "        new_x = robot_pose[0] + dx\n",
    "        new_y = robot_pose[1] + dy\n",
    "        new_z = robot_pose[2] + dz\n",
    "\n",
    "        if (0 <= new_x < max_x) and (0 <= new_y < max_y) and (0 <= new_z < max_z):\n",
    "            flag = False\n",
    "            return [new_x, new_y, new_z]\n",
    "        else:\n",
    "            action = random.randint(0, len(action_map) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45277848-b43a-4c84-9370-955be7a81f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_reward_3d(robot_pose, goal_pose, action, collided, reached_goal,\n",
    "                      previous_distance, current_distance,\n",
    "                      explored_cells, visited_set, occ_map, recent_pos):\n",
    "    reward = 0.0\n",
    "    rx, ry, rz = robot_pose\n",
    "    gx, gy, gz = goal_pose\n",
    "    dx, dy, dz = action\n",
    "\n",
    "    # Penalize repeated positions (loops)\n",
    "    if recent_pos.count(tuple(robot_pose)) >= 1:\n",
    "        reward -= 20 * recent_pos.count(tuple(robot_pose))\n",
    "\n",
    "    # Axis alignment and directed motion\n",
    "    if rx == gx:\n",
    "        reward += 200\n",
    "        y_diff = gy - ry\n",
    "        if (y_diff > 0 and dy == 1) or (y_diff < 0 and dy == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "        z_diff = gz - rz\n",
    "        if (z_diff > 0 and dz == 1) or (z_diff < 0 and dz == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "    if ry == gy:\n",
    "        reward += 200\n",
    "        x_diff = gx - rx\n",
    "        if (x_diff > 0 and dx == 1) or (x_diff < 0 and dx == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "        z_diff = gz - rz\n",
    "        if (z_diff > 0 and dz == 1) or (z_diff < 0 and dz == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "    if rz == gz:\n",
    "        reward += 200\n",
    "        x_diff = gx - rx\n",
    "        if (x_diff > 0 and dx == 1) or (x_diff < 0 and dx == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "        y_diff = gy - ry\n",
    "        if (y_diff > 0 and dy == 1) or (y_diff < 0 and dy == -1):\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward -= 100\n",
    "\n",
    "    # Terminal conditions\n",
    "    if collided:\n",
    "        return reward - 1000.0\n",
    "\n",
    "    if reached_goal:\n",
    "        return reward + 1000.0\n",
    "\n",
    "    # Boundary penalties and center reward (assuming size 60x60x60)\n",
    "    for i, val in enumerate([rx, ry, rz]):\n",
    "        if val < 3 or val > 56:\n",
    "            reward -= 20\n",
    "        else:\n",
    "            reward += 10\n",
    "\n",
    "    # Distance shaping\n",
    "    if current_distance < previous_distance:\n",
    "        reward += 20.0\n",
    "    else:\n",
    "        reward -= 20.0\n",
    "\n",
    "    # Time penalty\n",
    "    reward -= 5\n",
    "\n",
    "    # Closer-to-goal axis-wise reward\n",
    "    reward += (60 - abs(gx - rx)) + (60 - abs(gy - ry)) + (60 - abs(gz - rz))\n",
    "\n",
    "    # Exploration bonus\n",
    "    reward += 0.1 * explored_cells\n",
    "\n",
    "    # Lookahead for obstacle in 3D\n",
    "    is_clear = True\n",
    "    for step in range(1, 11):\n",
    "        nx = rx + dx * step\n",
    "        ny = ry + dy * step\n",
    "        nz = rz + dz * step\n",
    "\n",
    "        if (0 <= nx < occ_map.shape[0] and\n",
    "            0 <= ny < occ_map.shape[1] and\n",
    "            0 <= nz < occ_map.shape[2]):\n",
    "            if occ_map[nx, ny, nz] >= 1:\n",
    "                reward -= 10 * (10 - step)\n",
    "                is_clear = False\n",
    "                break\n",
    "        else:\n",
    "            reward -= 5  # out of bounds\n",
    "            break\n",
    "\n",
    "    if is_clear:\n",
    "        reward += 5\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8bda1-9ae0-4d79-8279-b73b72ae5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step_3d(state, action, goal, direction, huristic, explored, recent_pos):\n",
    "    \"\"\"\n",
    "    Simulates one environment step in a 3D grid.\n",
    "\n",
    "    Parameters:\n",
    "        state          : global 3D occupancy map (np.ndarray of shape [X, Y, Z])\n",
    "        action         : new robot position [x, y, z]\n",
    "        goal           : goal position [x, y, z]\n",
    "        direction      : movement direction (dx, dy, dz)\n",
    "        huristic       : previous distance to goal\n",
    "        explored       : set of visited positions\n",
    "        recent_pos     : deque of recent positions\n",
    "\n",
    "    Returns:\n",
    "        next_state     : local 3D map (1, 20, 20, 20, 1)\n",
    "        reward         : reward for the step\n",
    "        done           : whether the episode should terminate\n",
    "    \"\"\"\n",
    "    # --- Extract local 3D map ---\n",
    "    next_map = get_local_map_3d(state, action, size=20)\n",
    "    next_state = next_map.astype(np.float32).reshape(1, 20, 20, 20, 1)\n",
    "\n",
    "    # --- Collision check ---\n",
    "    x, y, z = action\n",
    "    collided = state[x, y, z] >= 1\n",
    "\n",
    "    # --- Goal check ---\n",
    "    reached_goal = (x == goal[0]) and (y == goal[1]) and (z == goal[2])\n",
    "\n",
    "    # --- Distance update ---\n",
    "    previous_distance = huristic\n",
    "    current_distance = np.linalg.norm(np.array(goal) - np.array(action))\n",
    "\n",
    "    # --- Compute reward ---\n",
    "    reward = compute_reward_3d(\n",
    "        robot_pose=action,\n",
    "        goal_pose=goal,\n",
    "        action=direction,\n",
    "        collided=collided,\n",
    "        reached_goal=reached_goal,\n",
    "        previous_distance=previous_distance,\n",
    "        current_distance=current_distance,\n",
    "        explored_cells=10,\n",
    "        visited_set=explored,\n",
    "        occ_map=state,\n",
    "        recent_pos=recent_pos\n",
    "    )\n",
    "\n",
    "    # --- Terminate if goal reached or collided ---\n",
    "    done = reached_goal or collided\n",
    "\n",
    "    return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caf7f5-0de0-43a3-ab95-f488f01e979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "MAX_STEPS_EP = 3600\n",
    "\n",
    "global_map = np.zeros((60, 60, 60), dtype=np.int8)\n",
    "global_map[10:15, 10:50,0:40] = 1\n",
    "global_map[30:35, 5:55 ,0:20] = 1\n",
    "global_map[41:42, 41:42 , 0:55] = 1\n",
    "\n",
    "robot_goal_map = {\n",
    "    0: ((5, 5, 5), (8, 55, 45)),\n",
    "    1: ((5, 5, 5), (50, 3, 10)),\n",
    "    2: ((5, 5, 5), (25, 25, 25)),\n",
    "    3: ((8, 8, 8), (38, 42, 12)),\n",
    "    4: ((5, 55, 5), (50, 20, 30)),\n",
    "    5: ((25, 25, 25), (5, 5, 5)),\n",
    "    6: ((8, 30, 10), (45, 10, 20)),\n",
    "    7: ((55, 55, 55), (20, 10, 10)),\n",
    "    8: ((50, 30, 10), (5, 30, 45)),\n",
    "    9: ((3, 40, 50), (40, 3, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "action_map = {\n",
    "    0: (-1, 0, 0),  # up x\n",
    "    1: (1, 0, 0),   # down x\n",
    "    2: (0, -1, 0),  # left y\n",
    "    3: (0, 1, 0),   # right y\n",
    "    4: (0, 0, -1),  # down z\n",
    "    5: (0, 0, 1),   # up z\n",
    "}\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # --- Reset environment ---\n",
    "    #print(robot_pose)\n",
    "    pair_id = episode // 100\n",
    "    robot_pose, goal_pose = robot_goal_map[pair_id]\n",
    "    print(\"pair id: \" + str(pair_id) + \" robo: \" + str(robot_pose) + \" goal: \" + str(goal_pose))\n",
    "    robot_pose = list(robot_pose)\n",
    "    goal_pose = list(goal_pose)\n",
    "    recent_pos = deque(maxlen=20)\n",
    "    local_map = get_local_map_3d(global_map, robot_pose, size=20)\n",
    "    state_grid = local_map.astype(np.float32).reshape(1, 20, 20, 20, 1)\n",
    "    state_pose = np.array([[robot_pose[0], robot_pose[1], robot_pose[2],goal_pose[0], goal_pose[1], goal_pose[2]]], dtype=np.float32)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    #print(\"rbpose\",robot_pose)\n",
    "    step_count = 0\n",
    "    while not done and step_count<=MAX_STEPS_EP:\n",
    "        step_count += 1\n",
    "        # --- Epsilon-greedy action selection ---\n",
    "        # if np.random.rand() <= EPSILON:\n",
    "        #     action = random.randint(0, ACTION_SIZE - 1)\n",
    "        # else:\n",
    "        #     q_vals = policy_net.predict([state_grid[np.newaxis, ...], state_pose[np.newaxis, ...]], verbose=0)\n",
    "        #     action = np.argmax(q_vals[0])\n",
    "\n",
    "        # --- Take action in environment ---\n",
    "        #print(\"current\", robot_pose)\n",
    "        huristic = np.sqrt(np.square(goal_pose[0] - robot_pose[0]) + np.square(goal_pose[1] - robot_pose[1]))\n",
    "        q_values = policy_net.predict([state_grid, state_pose], verbose=0)\n",
    "        action = np.argmax(q_values[0])\n",
    "        move = action_map[action]\n",
    "       # print(move)\n",
    "       # robot_pose[0] = robot_pose[0] + move[0]\n",
    "       # robot_pose[1] = robot_pose[1] + move[1]\n",
    "        robot_pose = safe_move_3d(robot_pose,action_map,action,60)\n",
    "        recent_pos.append(tuple(robot_pose))\n",
    "       # next_grid, reward, done = env_step(global_map, robot_pose,goal_pose,move,huristic,explored,recent_pos)  # Placeholder\n",
    "       # next_pose = np.array([[robot_pose[0], robot_pose[1], goal_pose[0], goal_pose[1]]], dtype=np.float32)\n",
    "        next_grid, reward, done = env_step_3d(global_map, robot_pose, goal_pose, move,huristic,10, recent_pos)\n",
    "        next_pose = np.array([[robot_pose[0], robot_pose[1], robot_pose[2],goal_pose[0], goal_pose[1], goal_pose[2]]], dtype=np.float32)\n",
    "        #print(robot_pose,reward)\n",
    "        memory.append((state_grid, state_pose, action, reward, next_grid, next_pose, done))\n",
    "\n",
    "        state_grid, state_pose = next_grid, next_pose\n",
    "        total_reward += reward\n",
    "\n",
    "        # --- Train if enough samples ---\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            minibatch = random.sample(memory, BATCH_SIZE)\n",
    "\n",
    "            grids = np.array([x[0] for x in minibatch])\n",
    "            poses = np.array([x[1] for x in minibatch])\n",
    "            actions = np.array([x[2] for x in minibatch])\n",
    "            rewards = np.array([x[3] for x in minibatch])\n",
    "            next_grids = np.array([x[4] for x in minibatch])\n",
    "            next_poses = np.array([x[5] for x in minibatch])\n",
    "            dones = np.array([x[6] for x in minibatch])\n",
    "\n",
    "          #  print(\"grids shape:\", grids.shape)   # Should be (batch_size, 20, 20, 1)\n",
    "           # print(\"poses shape:\", poses.shape)   # Should be (batch_size, 4)\n",
    "\n",
    "            q_targets = policy_net.predict([grids.reshape(-1, 20, 20, 20, 1),poses.reshape(-1, 6)], verbose=0)\n",
    "            q_next = target_net.predict([next_grids.reshape(-1, 20, 20, 20, 1),next_poses.reshape(-1, 6)], verbose=0)\n",
    "            #policy_net.fit([grids.reshape(-1, 20, 20, 20, 1),poses.reshape(-1, 6)], q_targets, verbose=0)\n",
    "\n",
    "\n",
    "           # q_targets = policy_net.predict([grids.reshape(-1,20,20,1), poses.reshape(-1,4)], verbose=0)\n",
    "            #q_next = target_net.predict([next_grids.reshape(-1,20,20,1), next_poses.reshape(-1,4)], verbose=0)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                target = rewards[i]\n",
    "                if not dones[i]:\n",
    "                    target += GAMMA * np.amax(q_next[i])\n",
    "                q_targets[i][actions[i]] = target\n",
    "\n",
    "            # --- Train policy network ---\n",
    "            policy_net.fit([grids.reshape(-1, 20, 20, 20, 1),poses.reshape(-1, 6)], q_targets, verbose=0)\n",
    "\n",
    "    # --- Update target network ---\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "    # --- Epsilon decay ---\n",
    "    if EPSILON > EPSILON_MIN:\n",
    "        EPSILON *= EPSILON_DECAY\n",
    "    if episode % 50 == 0:\n",
    "        policy_net.save(f\"/storage/projects2/e19-4yp-g28-peraswarm-local-nav/checkpoint_ep_drone{episode}.h5\")\n",
    "        print(f\" Saved checkpoint at episode {episode}\")\n",
    "    print(f\"Episode {episode+1}/{EPISODES} - Total reward: {total_reward:.2f} - Epsilon: {EPSILON:.2f}\")\n",
    "    \n",
    "policy_net.save(\"path_planing_model_3d_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04ddde-228d-4c02-8bba-ad04d491936e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
